# -*- coding: utf-8 -*-
"""ViT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SJZC5xCfp4Tb7j3glS91tWgxUfwfKVWE
"""

!pip install -U tensorflow-addons

import numpy as np
import matplotlib.pyplot as plt
import cv2

import tensorflow as tf
import tensorflow_addons as tfa
from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.utils import plot_model

img = np.zeros((3, 4, 4, 1))

img[0][..., 0] = np.array([[69, 72, 70, 72],
                [63, 96, 71, 70],
                [44, 158, 37, 24],
                [37, 52, 165, 106]])

img[1][..., 0] = np.array([[157, 162, 152, 167],
                [138, 157, 136, 137],
                [123, 128, 99, 27],
                [115, 116, 167, 97]])

img[2][..., 0] = np.array([[138, 33, 211, 233],
                [195, 8, 83, 153],
                [235, 161, 12, 72],
                [165, 170, 35, 80]])

y = np.array([0, 1, 2])
labels = ["Cat", "Dog", "Bird"]

learning_rate = 0.001
weight_decay = 0.0001
batch_size = 256
num_epochs = 5#10#0
image_size = 4
patch_size = 2
num_patches = (image_size // patch_size) ** 2
projection_dim = 4#3#16#64
num_heads = 4 # 4 different parameterization of the query, key and value matrices

transformer_units = [projection_dim * 2, projection_dim]
transformer_layers = 1#2#4#8
mlp_head_units = [4, 2]

num_train_images = img.shape[0]

class Patches(layers.Layer):
	def __init__(self, patch_size):
		super(Patches, self).__init__()
		self.patch_size = patch_size

	def call(self, images):
		batch_size = tf.shape(images)[0]
		patches = tf.image.extract_patches(images = images, sizes = [1, self.patch_size, self.patch_size, 1], strides = [1, self.patch_size, self.patch_size, 1], rates = [1, 1, 1, 1], padding = "VALID")

		patch_dims = patches.shape[-1]
		patches = tf.reshape(patches, [batch_size, -1, patch_dims])
		tf.print("\nThe patches: \n\nOriginal Form\n", patches, summarize = -1)
		for i in range(len(patches)):
			tf.print("\nImage", i+1)
			for j in range(len(patches[0])):
				tf.print("Patch", j+1)
				tf.print(patches[i, j], summarize = -1)
		return patches

patches = Patches(patch_size)(img)

class PatchEncoder(layers.Layer):
	def __init__(self, num_patches, projection_dim):
		super(PatchEncoder, self).__init__()
		self.num_patches = num_patches
		self.projection = layers.Dense(units = projection_dim)
		self.position_embedding = layers.Embedding(input_dim=num_patches, output_dim=projection_dim)
	def call(self, patch):
		positions = tf.range(start=0, limit=self.num_patches, delta=1)
		tf.print("\Patch Positions:", positions, "\n")
		projection = self.projection(patch)
		position_embedding = self.position_embedding(positions)
		encoded = projection + position_embedding
		tf.print("\nWeights of Dense Layer: \n", self.projection.kernel, summarize = -1)
		tf.print("\nBiases of Dense Layer: \n", self.projection.bias, summarize = -1)
		tf.print("\Shape of patch: ", patch.shape)
		tf.print("\n Weights of Embedding Layer: \n", self.position_embedding.weights, summarize=-1)
		tf.print("\nAfter Applying projection (layers.Dense) on patches:\n------\n", projection, summarize = -1)
		tf.print("\nProjection Shape: ", projection.shape)
		tf.print("\n\nAfter Applying position_embedding(layers.Embedding) on the patch positions: \n------\n", position_embedding, summarize = -1)
		tf.print("\nPosition Encoding Shape: ", position_embedding.shape)
		tf.print("\n\nEncoded Patches got After Adding Positional Encoding with Projected Patches:\n-----\n", encoded, "\n", summarize = -1)
		return encoded

encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)

x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)

attention_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=projection_dim, dropout=0.1)(x1, x1)

x2 = layers.Add()([attention_output, encoded_patches])

x3 = layers.LayerNormalization(epsilon=1e-6)(x2)

GeluDense = layers.Dense(units = 8, activation = tf.nn.gelu)
x3_1 = GeluDense(x3)

Drop0 = layers.Dropout(rate = 0.1)
x3_2 = Drop0(x3_1, training = True)

GeluDense2 = layers.Dense(units = 4, activation = tf.nn.gelu)
x3_3 = GeluDense2(x3_2)

Drop02 = layers.Dropout(rate = 0.1)
x3_4 = Drop02(x3_3, training = True)

encoded_patches = layers.Add()([x3, x2])

representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)

representation = layers.Flatten()(representation)

representation = layers.Dropout(0.5)(representation)

def mlp(x, hidden_units, dropout_rate):
  for units in hidden_units:
    MLPDense = layers.Dense(units, activation = tf.nn.gelu)
    x=MLPDense(x)
    if tfa:
      tf.print("\nWeights of Dense Layer with ", units, "units:\n", MLPDense.kernel, summarize = -1)
      tf.print("\nBiases of Dense Layer with ", units, "units:\n", MLPDense.bias, summarize = -1)
      tf.print("\nAfter applying mlp Dense Layer with ", units, "units:\n", x, summarize = -1)
    x = layers.Dropout(dropout_rate)(x)
    if tfa:
      tf.print("\nAfter applying Dropout:\n", x, summarize = -1)
  return x

features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)

dense_layer_for_logits = layers.Dense(num_train_images)
logits = dense_layer_for_logits(features)

softmax_layer = layers.Softmax()
softmaxed_logits = softmax_layer(logits)

def create_vit_classifier():
  inputs = layers.Input(shape=img[0].shape) #, dtype="uint8")
  patches = Patches(patch_size)(inputs)
  encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)
  for _ in range(transformer_layers):
    x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)
    attention_output  = layers.MultiHeadAttention(num_heads=num_heads, key_dim=projection_dim, dropout=0.1)(x1, x1)
    x2 = layers.Add()([attention_output, encoded_patches])
    x3 = layers.LayerNormalization(epsilon=1e-6)(x2)
    x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)
    encoded_patches = layers.Add()([x3, x2])
  representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)
  representation = layers.Flatten()(representation)
  representation = layers.Dropout(0.5)(representation)
  features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)
  logits = layers.Dense(num_train_images)(features)
  softmaxed_logits = layers.Softmax()(logits)
  model = keras.Model(inputs=inputs, outputs=softmaxed_logits)
  return model

optimizer = tfa.optimizers.AdamW(learning_rate=learning_rate, weight_decay=weight_decay)

tfa = False

model = create_vit_classifier()

model.compile(
    optimizer=optimizer,
    loss=keras.losses.SparseCategoricalCrossentropy(from_logits = False #True
    ),
    metrics=[
        keras.metrics.SparseCategoricalAccuracy(name="accuracy"),
    ],
)

plot_model(model, to_file = "TheTransformer.png", show_shapes = True, show_layer_names = True, show_dtype = True, show_layer_activations = True)

model.fit(x=img, y=y, batch_size=batch_size, epochs=5)

predictions = model.predict(img)
tf.print("Prediction:\n", predictions, summarize = -1)

print("Predicting Using The Logits\n")
for i, prediction in enumerate(predictions):
  predicted_index = np.argmax(prediction)
  print(f'Predicted on Image {i+1}: {labels[predicted_index]}')